## State of the art and expected progress beyond state of the art
The ultimate goal of the D2K process is to obtain knowledge that can be used to solve data-driven problems in a variety of situations. Much research in machine learning and data mining has been devoted to coping with an ever-growing variety of data and models, from kernel methods addressing high-dimensional data to relational methods tackling highly structured data. In practice this means that given a problem, the best possible selection of available techniques is performed, where this choice is based on how well the model performs on the problem at hand. This commonly results in overfitting to the training context, which restricts model applicability whenever the training conditions change. As a result, models must be discarded and retrained repeatedly, which is not cost-effective.

Machine learning has addressed this problem in limited ways. One common way is the use or assumption of a more general data distribution: either theoretically or empirically, e.g., by the use of additional data covering a broader space (semi-supervised learning). However, a general model is not necessarily good for adjusting well to particular situations again. One reason for this is that a general model may not be ready for using information about each new specific context where it is meant to be applied. Crucially, in many situations we do have information about the new context. An advanced approach to model exploitation would use this information to adjust the deployment of a model, e.g., by modifying the decision rules, resulting in a quantum leap in terms of reusability and flexibility. The current state of the art in machine learning, however, only allows for an appropriate use of this information in a quite restricted set of scenarios, basically cost functions for the output attribute and distribution changes for the input and output attributes, as we briefly review now.

Context change in machine learning has been studied primarily through operating conditions defined by cost/utility matrices and class imbalances. For instance, if a classification model was trained for a uniform class distribution, there are techniques to adapt the model to other class distributions or skews. The tools for doing this in a classification task are now commonplace and are mainly based on ROC analysis [F10] or cost-sensitive learning [LS10]. These tools allow the knowledge engineer to (i) determine optimal operating points under the given operating condition; (ii) identify regions of optimality where one model dominates others; (iii) link the model scores to actual operating conditions in the domain through calibration; and (iv) construct optimal hybrid models that combine the dominance regions of an ensemble. To achieve all this, the notion of a model was extended from a crisp classifier to one outputting ranks or scores. Furthermore, many evaluation metrics and loss functions can be understood as an aggregation of costs in a range of operating conditions [HFF11]. The tools and theory around these concepts have significantly helped to build and apply models with a much better adjustment to costs, utilities and class frequencies.

Transfer learning [PY10] is a closely related and diverse area where emphasis is put on a change of distribution and also on whether (or in which proportions) examples are labelled between the source learning task and the target learning task. In addition, the notion of feature-representation transfer has also been analysed, but models are not conceived to be more versatile or general a priori. Similarly, model or theory revision delays the problem to deployment time and undertakes a change of the model, while what we want (and lack) is that the original model need not be dismissed and substituted by the new one, but just deployed in the appropriate way for the new situation. In other words, we do not want to build a set of models for a range of operating contexts or a very general, but inflexible, model, but rather a single versatile model which can be properly deployed in a range of operating contexts using all the available information about each context. Any advance in this regard, as the objectives of this project propose, constitutes an important innovation over the current state of the art (see also Figure 1).

Not every kind of information about the training and deployment contexts is sufficiently formal or operational to be integrated into a more advanced and flexible model deployment scenario. While we do not discard the application of more complex and sophisticated kinds of context changes in the future, given the size of the consortium and the novelty of some of the key ideas the project relies on, we consider three strands which are better suited to develop a general notion of operating contexts working as ‘parameters’ for model reframing.

Changing inputs and outputs: One of these strands is given by the change in number, character and nature of the input and output attributes. Many systems can deal with missing data or with unreliable data for some examples, or attributes for which new values appear during deployment time, but are not able to treat overall contextual information about these issues. Similarly, we lack tools when outputs can change; e.g., we have new classes, or a classification problem turns into a regression problem, or vice versa. In some occasions, the problem may need to be inverted, i.e., the model can be used to get the inputs from the outputs. This has been explored under the notion of negotiable features [BFHR11] but there is an almost unlimited potential for further progress in terms of reframing, including the extension of performance evaluation with these changing problem arrangements.

Relational data and background knowledge: A powerful and well-grounded area where information about context can be effectively treated is when context is given by changes in secondary relations (linked to the main relations by many-to-one relationships), in new features or in background knowledge. The approach taken by Inductive Logic Programming and relational data mining [D10] is perhaps the most powerful and flexible. The use of ‘quantiles’ was introduced in [LLB09,BL11] in order to calibrate continuous attributes of objects having a many-to-one relationship to the main individual in an urban blocks classification problem [PSLBP11]. The logical setting also provides versatile mechanisms to deal with incomplete knowledge and missing data through nonmonotonic reasoning and abduction [FK00], where a model can behave differently when reframed to the new context with these techniques. In the relational setting, background knowledge can be effectively changed with immediate effect on how a model operates, clearly suggesting how reframing could be done. Other kinds of background knowledge, can be used, for instance through a clustering method [FGW10].

Hierarchical and multidimensional representation: Hierarchical data is one of the areas where the application of a model to different contexts is often required, and the state of the art is still very limited. Hierarchies are usually exploited in different ways depending on the domain. Models which are generated from one dimension and level cannot be used for other dimensions or levels. These problems appear in areas such as granular computing [LYZ02], hierarchical modelling, multidimensional databases, subgroup discovery, quantisation and discretisation. In general, some tools may be useful for extending models to other parts of a hierarchy, such as quantification [F08,BFHR10], transduction, calibration, co-learning, etc. For instance, quantification is a relatively new machine learning task that can be, in principle, generalised to involve the aggregation of heterogeneous sources of information (hierarchical or not) and very different kinds of models. Existing techniques for the integration of data mining and multidimensional data warehousing (e.g. prediction cubes [CCLR05]) do not consider any of these techniques, and are not based on more versatile models working across the hierarchy, but on partial models which are put together when navigating through the data cubes.

As we have seen, contexts are not only given by changes of costs or distributions. We lack then a generic set of techniques which consider other common contexts from which we can render any available information effective. For this we also lack the definition of more versatile models and flexible reframing operators which can use this information. Much progress is possible in any of the strands above in order to attain a much better model exploitation. A more fundamental advancement we expect is given by a better understanding of the essence of a cost-effective D2K process; how models must be constructed (in terms of the information they need to collect and produce) in order to allow for a more powerful and efficient model exploitation in different contexts.

